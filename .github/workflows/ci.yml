name: CI

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.9", "3.10", "3.11"]

    services:
      redis:
        image: redis
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        
    - name: Install Redis
      run: |
        sudo apt-get update
        sudo apt-get install -y redis-server
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-cov
        pip install -e .
        
    - name: Run tests with coverage
      run: |
        pytest --cov=ether --cov-report=xml
        
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: true

  benchmark:
    needs: test
    runs-on: ubuntu-latest
    
    services:
      redis:
        image: redis
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - uses: actions/checkout@v3
      with:
        fetch-depth: 2  # Needed to get previous commit
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"
        
    - name: Install Redis
      run: |
        sudo apt-get update
        sudo apt-get install -y redis-server
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        
    - name: Download previous benchmark results
      uses: actions/download-artifact@v3
      continue-on-error: true
      with:
        name: benchmark-results
        path: previous_benchmark
        
    - name: Run benchmark
      run: |
        python src/scripts/benchmark.py
        
    - name: Compare benchmarks
      id: benchmark-compare
      run: |
        python - <<EOF
        import json
        import os
        from pathlib import Path
        import sys
        
        def load_results(path):
            try:
                with open(path) as f:
                    return json.load(f)
            except:
                return None
        
        current = load_results('benchmark_results/results.json')
        previous = load_results('previous_benchmark/benchmark_results/results.json')
        
        if not current:
            print("::error::No current benchmark results found")
            sys.exit(1)
            
        if not previous:
            print("No previous benchmark results to compare against")
            sys.exit(0)
            
        # Compare results
        regression_threshold = 0.10  # 10% regression threshold
        regressions = []
        
        for curr, prev in zip(current['results'], previous['results']):
            for metric, value in curr['metrics'].items():
                prev_value = prev['metrics'][metric]
                if metric in ['messages_per_second', 'latency_ms']:
                    change = (value - prev_value) / prev_value
                    if change < -regression_threshold:
                        regressions.append({
                            'metric': metric,
                            'config': curr['config'],
                            'previous': prev_value,
                            'current': value,
                            'change': change * 100
                        })
        
        if regressions:
            message = "Performance regressions detected:\n"
            for reg in regressions:
                message += f"\n{reg['metric']}:\n"
                message += f"  Config: {reg['config']}\n"
                message += f"  Previous: {reg['previous']:.2f}\n"
                message += f"  Current: {reg['current']:.2f}\n"
                message += f"  Change: {reg['change']:.1f}%\n"
            
            print(f"::warning::Performance Regression::{message}")
            
            # Create GitHub step output
            with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                f.write(f"has_regression=true\n")
                f.write(f"regression_details<<EOF\n{message}\nEOF\n")
        EOF
        
    - name: Store benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: benchmark_results/
        
    - name: Send notification
      if: steps.benchmark-compare.outputs.has_regression == 'true'
      uses: actions/github-script@v6
      with:
        script: |
          const details = process.env.REGRESSION_DETAILS;
          
          await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: 'ðŸš¨ Performance Regression Detected',
            body: details,
            labels: ['performance', 'regression']
          });
      env:
        REGRESSION_DETAILS: ${{ steps.benchmark-compare.outputs.regression_details }}